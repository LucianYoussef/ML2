{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35roXDEMudbw"
      },
      "source": [
        "# GUC Clustering Project "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIiItKbYudb2"
      },
      "source": [
        "**Objective:** \n",
        "The objective of this project teach students how to apply clustering to real data sets\n",
        "\n",
        "The projects aims to teach student: \n",
        "* Which clustering approach to use\n",
        "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
        "* How to tune the parameters of each data approach\n",
        "* What is the effect of different distance functions (optional) \n",
        "* How to evaluate clustering approachs \n",
        "* How to display the output\n",
        "* What is the effect of normalizing the data \n",
        "\n",
        "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtHElDYdudb3"
      },
      "outputs": [],
      "source": [
        "# if plotnine is not installed in Jupter then use the following command to install it \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHS5ZoQudb4"
      },
      "source": [
        "Running this project require the following imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrueqJenudb5"
      },
      "outputs": [],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "# import seaborn as sns \n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import sklearn.preprocessing as prep\n",
        "# from sklearn.datasets import make_blobs\n",
        "# from plotnine import *   \n",
        "# # StandardScaler is a function to normalize the data \n",
        "# # You may also check MinMaxScaler and MaxAbsScaler \n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.neighbors import NearestNeighbors\n",
        "# from sklearn.cluster import DBSCAN\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.mixture import GaussianMixture\n",
        "# from sklearn.metrics import silhouette_score\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju2Zj6-nudb5"
      },
      "outputs": [],
      "source": [
        "# # helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
        "# def display_cluster(X,km=[],num_clusters=0):\n",
        "#     colors = plt.cm.tab20(np.linspace(0, 1, 20))  #List colors\n",
        "#     alpha = 0.5  #color obaque\n",
        "#     s = 20\n",
        "#     if num_clusters == 0:\n",
        "#         plt.scatter(X[:,0],X[:,1],c = colors[0],alpha = alpha,s = s)\n",
        "#     else:\n",
        "#         for i in range(num_clusters):\n",
        "#             plt.scatter(X[km==i,0],X[km==i,1],c = colors[i],alpha = alpha,s=s)\n",
        "#     plt.show()\n",
        "#             #plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i], marker = 'x', s = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZnIbT3Mudb6"
      },
      "source": [
        "# Multi Blob Data Set \n",
        "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
        "* Cluster the data set below using \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeSqG318udb7",
        "outputId": "078fad92-3073-4558-b1e8-f0acd8d85d34"
      },
      "outputs": [],
      "source": [
        "# plt.rcParams['figure.figsize'] = [8,8]\n",
        "# sns.set_style(\"whitegrid\")\n",
        "# sns.set_context(\"talk\")\n",
        "\n",
        "# n_bins = 6  \n",
        "# centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
        "# Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
        "#                   centers=centers, shuffle=False, random_state=42)\n",
        "# display_cluster(Multi_blob_Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDSIGjubudb8"
      },
      "source": [
        "### Kmeans \n",
        "* Use Kmeans with different values of K to cluster the above data \n",
        "* Display the outcome of each value of K \n",
        "* Plot distortion function versus K and choose the approriate value of k \n",
        "* Plot the silhouette_score versus K and use it to choose the best K \n",
        "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Kmeans for Multi_blob dataset\n",
        "# dist_thresh = range(2, 10)  # range from 2 to 10 clusters\n",
        "\n",
        "# distortions = []\n",
        "# silhouette_scores = []\n",
        "\n",
        "# for k in dist_thresh:\n",
        "#     kmeans = KMeans(n_clusters=k, init='k-means++', tol=0.0001, random_state=42)\n",
        "#     kmeans.fit(Multi_blob_Data)  # Fit KMeans model to your data\n",
        "#     distortions.append(kmeans.inertia_)  \n",
        "#     silhouette_avg = silhouette_score(Multi_blob_Data, kmeans.labels_)  \n",
        "#     silhouette_scores.append(silhouette_avg)  \n",
        "#     cluster_centers = kmeans.cluster_centers_\n",
        "#     labels = kmeans.predict(Multi_blob_Data)\n",
        "#     #display_cluster(Multi_blob_Data,labels,k)\n",
        "    \n",
        "\n",
        "# # Plot graph of distortion against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(dist_thresh, distortions, marker='o', linestyle='-')\n",
        "# plt.title('Distortion vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Distortion')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(dist_thresh)\n",
        "# plt.show()\n",
        "# # Plot graph of silhouette against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(dist_thresh, silhouette_scores, marker='o', linestyle='-')\n",
        "# plt.title('Silhouette Score vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(dist_thresh)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ne3KmtPudb9"
      },
      "outputs": [],
      "source": [
        "# kmeans = KMeans(n_clusters=5, init='k-means++', tol=0.0001, random_state=42)\n",
        "# kmeans.fit(Multi_blob_Data)\n",
        "# cluster_centers = kmeans.cluster_centers_\n",
        "# labels = kmeans.predict(Multi_blob_Data)\n",
        "# display_cluster(Multi_blob_Data,labels,5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE7dvpOAudb9"
      },
      "source": [
        "### Hierarchal Clustering\n",
        "* Use AgglomerativeClustering function to  to cluster the above data \n",
        "* In the  AgglomerativeClustering change the following parameters \n",
        "    * Affinity (use euclidean, manhattan and cosine)\n",
        "    * Linkage( use average and single )\n",
        "    * Distance_threshold (try different)\n",
        "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
        "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
        "* Record your observation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = AgglomerativeClustering(n_clusters=None ,affinity='euclidean', linkage='single', distance_threshold=0.9)\n",
        "# model.fit(Multi_blob_Data)\n",
        "# labels = model.labels_\n",
        "# k = model.n_clusters_\n",
        "# print(k)\n",
        "# print(labels)\n",
        "# display_cluster(Multi_blob_Data, labels, k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_dendogram(data, metric, method):\n",
        "#     #plotting dendogram\n",
        "#     Z = linkage(data, metric=metric ,method=method)\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     dendrogram(Z)\n",
        "#     plt.title(f'Dendrogram')\n",
        "#     plt.xlabel('Sample Index')\n",
        "#     plt.ylabel('Distance')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Hierarchal clustering\n",
        "# def Hierarchal(data, range, affinity, linkage):\n",
        "#     dist_thresh = range\n",
        "\n",
        "#     silhouette_scores = []\n",
        "#     n_clus = []\n",
        "\n",
        "#     for k in dist_thresh:\n",
        "#         model = AgglomerativeClustering(n_clusters=None ,affinity=affinity, linkage=linkage, distance_threshold= k)\n",
        "#         model.fit(data)\n",
        "#         labels = model.labels_\n",
        "#         clusters = model.n_clusters_\n",
        "#         n_clus.append(clusters)\n",
        "#         silhouette_avg = silhouette_score(data, labels)  # Calculate silhouette score\n",
        "#         silhouette_scores.append(silhouette_avg)  # Store silhouette score\n",
        "#     #display_cluster(Multi_blob_Data, labels, n_clus)\n",
        "\n",
        "#     #plotting silhouette graph \n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.plot(dist_thresh, silhouette_scores, marker='o', linestyle='-')\n",
        "#     plt.title('Silhouette Score vs. Distance Thresholds')\n",
        "#     plt.xlabel('Distance Thresholds')\n",
        "#     plt.ylabel('Silhouette Score')\n",
        "#     plt.grid(True)\n",
        "#     plt.xticks(dist_thresh)\n",
        "#     plt.show()\n",
        "\n",
        "#     #plotting number of clusters vs threshold\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.plot(dist_thresh, n_clus, marker='o', linestyle='-')\n",
        "#     plt.title('Number of clusters vs. Distance Thresholds')\n",
        "#     plt.xlabel('Distance Thresholds')\n",
        "#     plt.ylabel('Number of clusters')\n",
        "#     plt.grid(True)\n",
        "#     plt.xticks(dist_thresh)\n",
        "#     plt.show()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Euclidean Distance\n",
        "\n",
        "# # Euclidean and average\n",
        "# plot_dendogram(Multi_blob_Data, 'euclidean', 'average')\n",
        "# Hierarchal(Multi_blob_Data, np.arange(1, 9, 1), 'euclidean', 'average' )\n",
        "\n",
        "# # Euclidean and single\n",
        "# plot_dendogram(Multi_blob_Data, 'euclidean', 'single')\n",
        "# Hierarchal(Multi_blob_Data, np.arange(0.1, 2.1, 0.1), 'euclidean', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Manhattan Distance\n",
        "\n",
        "# # Manhattan and average\n",
        "# plot_dendogram(Multi_blob_Data, 'cityblock', 'average')\n",
        "# Hierarchal(Multi_blob_Data, np.arange(1, 11, 1), 'manhattan', 'average' )\n",
        "\n",
        "# # Manhattan and single\n",
        "# plot_dendogram(Multi_blob_Data, 'cityblock', 'single')\n",
        "# Hierarchal(Multi_blob_Data, np.arange(0.1, 2.4, 0.1), 'manhattan', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cosine Distance\n",
        "\n",
        "# # Cosine and average\n",
        "# plot_dendogram(Multi_blob_Data, 'cosine', 'average')\n",
        "# Hierarchal(Multi_blob_Data, np.arange(0.1, 1.6, 0.1), 'cosine', 'average' )\n",
        "\n",
        "# # Cosine and single\n",
        "# plot_dendogram(Multi_blob_Data, 'cosine', 'single')\n",
        "# Hierarchal(Multi_blob_Data, np.arange(0.001, 0.01, 0.001), 'cosine', 'single' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myJE7vQKudb-"
      },
      "source": [
        "### DBScan\n",
        "* Use DBScan function to  to cluster the above data \n",
        "* In the  DBscan change the following parameters \n",
        "    * EPS (from 0.1 to 3)\n",
        "    * Min_samples (from 5 to 25)\n",
        "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
        "* Plot the resulting Clusters in this case \n",
        "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
        "* Record your observations and comments "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define range of parameters\n",
        "# eps_arr = np.arange(0.1, 3.1, 0.2)\n",
        "# min_samp = np.arange(5, 26, 5)\n",
        "\n",
        "# # Iterate over parameter combinations\n",
        "# for eps in eps_arr:\n",
        "#     for min_samples in min_samp:\n",
        "#         # Create a new figure for each combination\n",
        "#         plt.figure(figsize=(8, 6))\n",
        "\n",
        "#         # Perform DBSCAN clustering\n",
        "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "#         labels = dbscan.fit_predict(Multi_blob_Data)\n",
        "#         unique_labels = np.unique(labels)\n",
        "#         clusters = len(unique_labels) - 1\n",
        "#         # Check if there are at least 2 clusters\n",
        "#         if len(unique_labels) > 1:\n",
        "#             # Calculate silhouette score\n",
        "#             score = silhouette_score(Multi_blob_Data, labels)\n",
        "\n",
        "#             # Plot clusters\n",
        "#             colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
        "#             for k, color in zip(unique_labels, colors):\n",
        "#                 class_member_mask = (labels == k)\n",
        "#                 xy = Multi_blob_Data[class_member_mask]\n",
        "#                 plt.scatter(xy[:, 0], xy[:, 1], c=[color], s=10)\n",
        "\n",
        "#             plt.title(f'EPS={eps}, Min Samples={min_samples}, Cluster={clusters}\\nSilhouette Score: {score:.2f}\\nlables={unique_labels}')\n",
        "#             plt.colorbar(label='Cluster Label')\n",
        "#             plt.grid(True)\n",
        "#             plt.show()\n",
        "            \n",
        "# print(\"Best silhouette score:\", best_score)\n",
        "# print(\"Best parameters:\", best_params)\n",
        "# silhouette_scores = np.array(silhouette_scores)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.scatter(silhouette_scores[:, 0], silhouette_scores[:, 1], c=silhouette_scores[:, 2], cmap='viridis')\n",
        "# plt.colorbar(label='Silhouette Score')\n",
        "# plt.xlabel('EPS')\n",
        "# plt.ylabel('Min Samples')\n",
        "# plt.title('Silhouette Score vs. EPS and Min Samples')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# print(\"Best silhouette score:\", best_score)\n",
        "# print(\"Best parameters:\", best_params)\n",
        "# # Plot clusters using best parameters\n",
        "# dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "# best_labels = dbscan.fit_predict(Multi_blob_Data)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=best_labels, cmap='viridis', alpha=0.7, s=50)\n",
        "# plt.title('DBSCAN Clustering with Best Parameters')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "# plt.colorbar(label='Cluster Label')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiQtpAt5udb_"
      },
      "outputs": [],
      "source": [
        "# esp_arr = np.arange(0.1, 3.1, 0.2)  \n",
        "# min_samp = np.arange(5, 26, 5)  \n",
        "\n",
        "# for esp in esp_arr:\n",
        "#     for samp in min_samp:\n",
        "#         dbscan = DBSCAN(eps=esp, min_samples=samp)\n",
        "#         dbscan.fit(Multi_blob_Data)\n",
        "#         labels = dbscan.labels_\n",
        "#         print(labels)\n",
        "#         unique_labels = np.unique(labels)\n",
        "#         print(unique_labels)\n",
        "#         k = len(unique_labels) - 1\n",
        "#         print(k)\n",
        "#         if k < 21:\n",
        "#             #display_cluster(Multi_blob_Data, labels, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip16g1QFudb_"
      },
      "source": [
        "### Gaussian Mixture\n",
        "* Use GaussianMixture function to cluster the above data \n",
        "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
        "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Fit a Gaussian Mixture Model with different covariance types\n",
        "# covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "# n_comp = range(1,8)\n",
        "\n",
        "\n",
        "# for comp in n_comp:\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "#     for i, cov_type in enumerate(covariance_types):\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "#         gmm.fit(Multi_blob_Data)\n",
        "\n",
        "#         # Create grid to evaluate model\n",
        "#         x = np.linspace(-2, 12)\n",
        "#         y = np.linspace(-2, 10)\n",
        "#         X_grid, Y_grid = np.meshgrid(x, y)\n",
        "#         XX = np.array([X_grid.ravel(), Y_grid.ravel()]).T\n",
        "#         Z = -gmm.score_samples(XX)\n",
        "#         Z = Z.reshape(X_grid.shape)\n",
        "\n",
        "#         # Plot contour\n",
        "#         plt.subplot(2, 2, i+1)\n",
        "#         plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], s=10, c=gmm.predict(Multi_blob_Data), cmap='viridis', zorder=2)\n",
        "#         plt.contour(X_grid, Y_grid, Z, levels=10, linewidths=1, colors='black', zorder=1)\n",
        "#         plt.title('GMM with Covariance Type: ' + cov_type)\n",
        "\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m92lZkkyudb_"
      },
      "source": [
        "# Iris data set \n",
        "The iris data set is test data set that is part of the Sklearn module \n",
        "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
        "\n",
        "The data represents three classes \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QaCWyyCudcA",
        "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
      },
      "outputs": [],
      "source": [
        "# from sklearn.datasets import load_iris\n",
        "# iris_data = load_iris()\n",
        "# iris_data.target[[10, 25, 50]]\n",
        "# #array([0, 0, 1])\n",
        "# list(iris_data.target_names)\n",
        "# ['setosa', 'versicolor', 'virginica']\n",
        "# dataframe = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n",
        "# display_multi(dataframe, iris_data.target, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unnormalised Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #KMeans for iris dataset\n",
        "\n",
        "# dist_thresh = range(2, 10)  # range from 2 to 10 clusters\n",
        "\n",
        "# distortions = []\n",
        "# silhouette_scores = []\n",
        "\n",
        "# for k in dist_thresh:\n",
        "#     kmeans = KMeans(n_clusters=k, init='k-means++', tol=0.0001, random_state=45)\n",
        "#     kmeans.fit(iris_data.data)  # Fit KMeans model\n",
        "#     distortions.append(kmeans.inertia_) \n",
        "#     silhouette_avg = silhouette_score(iris_data.data, kmeans.labels_)  \n",
        "#     silhouette_scores.append(silhouette_avg) \n",
        "#     cluster_centers = kmeans.cluster_centers_\n",
        "#     labels = kmeans.predict(iris_data.data)\n",
        "    \n",
        "\n",
        "# # Plot graph of distortion against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(dist_thresh, distortions, marker='o', linestyle='-')\n",
        "# plt.title('Distortion vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Distortion')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(dist_thresh)\n",
        "# plt.show()\n",
        "# # Plot graph for silhouette against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(dist_thresh, silhouette_scores, marker='o', linestyle='-')\n",
        "# plt.title('Silhouette Score vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(dist_thresh)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Euclidean Distance\n",
        "\n",
        "# # Euclidean and average\n",
        "# plot_dendogram(iris_data.data, 'euclidean', 'average')\n",
        "# Hierarchal(iris_data.data, np.arange(1, 4.5, 0.5), 'euclidean', 'average' )\n",
        "\n",
        "# # Euclidean and single\n",
        "# plot_dendogram(iris_data.data, 'euclidean', 'single')\n",
        "# Hierarchal(iris_data.data, np.arange(0.1, 1.7, 0.2), 'euclidean', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Manhattan Distance\n",
        "\n",
        "# # Manhattan and average\n",
        "# plot_dendogram(iris_data.data, 'cityblock', 'average')\n",
        "# Hierarchal(iris_data.data, np.arange(1, 7, 1), 'manhattan', 'average' )\n",
        "\n",
        "# # Manhattan and single\n",
        "# plot_dendogram(iris_data.data, 'cityblock', 'single')\n",
        "# Hierarchal(iris_data.data, np.arange(0.1, 2.4, 0.1), 'manhattan', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cosine Distance\n",
        "\n",
        "# # Cosine and average\n",
        "# plot_dendogram(iris_data.data, 'cosine', 'average')\n",
        "# Hierarchal(iris_data.data, np.arange(0.001, 0.02, 0.001), 'cosine', 'average' )\n",
        "\n",
        "# # Cosine and single\n",
        "# plot_dendogram(iris_data.data, 'cosine', 'single')\n",
        "# Hierarchal(iris_data.data, np.arange(0.001, 0.006, 0.001), 'cosine', 'single' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# # Load iris dataset\n",
        "# iris_data = load_iris()\n",
        "\n",
        "# covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "# n_comp = range(1, 8)\n",
        "\n",
        "# for comp in n_comp:\n",
        "#     for i, cov_type in enumerate(covariance_types):\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "\n",
        "#         for feature_idx in range(4):\n",
        "#             for other_idx in range(4):\n",
        "#                 if feature_idx != other_idx and other_idx > feature_idx:\n",
        "#                     # Create new figure for each pair of features\n",
        "#                     plt.figure(figsize=(8, 6))\n",
        "\n",
        "#                     # Fit the Gaussian Mixture Model\n",
        "#                     gmm.fit(iris_data.data[:, [feature_idx, other_idx]])\n",
        "\n",
        "#                     # Create grid to evaluate model\n",
        "#                     x = np.linspace(iris_data.data[:, feature_idx].min() - 1, iris_data.data[:, feature_idx].max() + 1)\n",
        "#                     y = np.linspace(iris_data.data[:, other_idx].min() - 1, iris_data.data[:, other_idx].max() + 1)\n",
        "#                     X, Y = np.meshgrid(x, y)\n",
        "#                     XX = np.column_stack([X.ravel(), Y.ravel()])\n",
        "\n",
        "#                     # Score samples and reshape\n",
        "#                     Z = -gmm.score_samples(XX)\n",
        "#                     Z = Z.reshape(X.shape)\n",
        "\n",
        "#                     # Plot contour\n",
        "#                     plt.scatter(iris_data.data[:, feature_idx], iris_data.data[:, other_idx], s=40, c=gmm.predict(iris_data.data[:, [feature_idx, other_idx]]), cmap='viridis', zorder=2)\n",
        "#                     plt.contour(X, Y, Z, levels=10, linewidths=1, colors='black', zorder=1)\n",
        "#                     plt.title(f'GMM with Covariance Type: {cov_type}\\nFeatures: {iris_data.feature_names[feature_idx]} vs {iris_data.feature_names[other_idx]}')\n",
        "#                     plt.xlabel(iris_data.feature_names[feature_idx])\n",
        "#                     plt.ylabel(iris_data.feature_names[other_idx])\n",
        "                    \n",
        "#                     plt.tight_layout()\n",
        "#                     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# silhouette_scores = []\n",
        "# n_comp = range(2, 8)\n",
        "# for comp in n_comp:\n",
        "#     comp_scores = []\n",
        "#     for cov_type in covariance_types:\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "#         features_data = iris_data.data[:, :7]  # Assuming first 7 columns are features\n",
        "\n",
        "#         # Calculate silhouette score\n",
        "#         silhouette_avg = silhouette_score(features_data, gmm.fit_predict(features_data))\n",
        "#         comp_scores.append(silhouette_avg)\n",
        "\n",
        "#     silhouette_scores.append(comp_scores)\n",
        "\n",
        "# # Print silhouette scores\n",
        "# for i, comp in enumerate(n_comp):\n",
        "#     for j, cov_type in enumerate(covariance_types):\n",
        "#         print(f\"Number of components: {comp}, Covariance type: {cov_type}, Silhouette score: {silhouette_scores[i][j]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define range of parameters\n",
        "# eps_arr = np.arange(0.1, 3.1, 0.2)\n",
        "# min_samp = np.arange(5, 26, 5)\n",
        "\n",
        "# # Iterate over parameter combinations\n",
        "# for eps in eps_arr:\n",
        "#     for min_samples in min_samp:\n",
        "#         # Create a new figure for each combination\n",
        "#         plt.figure(figsize=(8, 6))\n",
        "\n",
        "#         # Perform DBSCAN clustering\n",
        "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "#         labels = dbscan.fit_predict(Multi_blob_Data)\n",
        "#         unique_labels = np.unique(labels)\n",
        "#         clusters = len(unique_labels) - 1\n",
        "#         # Check if there are at least 2 clusters\n",
        "#         if len(unique_labels) > 1:\n",
        "#             # Calculate silhouette score\n",
        "#             score = silhouette_score(Multi_blob_Data, labels)\n",
        "\n",
        "#             # Plot clusters\n",
        "#             colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
        "#             for k, color in zip(unique_labels, colors):\n",
        "#                 class_member_mask = (labels == k)\n",
        "#                 xy = Multi_blob_Data[class_member_mask]\n",
        "#                 plt.scatter(xy[:, 0], xy[:, 1], c=[color], s=10)\n",
        "\n",
        "#             plt.title(f'EPS={eps}, Min Samples={min_samples}, Cluster={clusters}\\nSilhouette Score: {score:.2f}\\nlables={unique_labels}')\n",
        "#             plt.colorbar(label='Cluster Label')\n",
        "#             plt.grid(True)\n",
        "#             plt.show()\n",
        "            \n",
        "# print(\"Best silhouette score:\", best_score)\n",
        "# print(\"Best parameters:\", best_params)\n",
        "# silhouette_scores = np.array(silhouette_scores)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.scatter(silhouette_scores[:, 0], silhouette_scores[:, 1], c=silhouette_scores[:, 2], cmap='viridis')\n",
        "# plt.colorbar(label='Silhouette Score')\n",
        "# plt.xlabel('EPS')\n",
        "# plt.ylabel('Min Samples')\n",
        "# plt.title('Silhouette Score vs. EPS and Min Samples')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# print(\"Best silhouette score:\", best_score)\n",
        "# print(\"Best parameters:\", best_params)\n",
        "# # Plot clusters using best parameters\n",
        "# dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "# best_labels = dbscan.fit_predict(Multi_blob_Data)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=best_labels, cmap='viridis', alpha=0.7, s=50)\n",
        "# plt.title('DBSCAN Clustering with Best Parameters')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "# plt.colorbar(label='Cluster Label')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalised Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyoCVfyMudcA"
      },
      "source": [
        "* Repeat all the above clustering approaches and steps on the above data \n",
        "* Normalize the data then repeat all the above steps \n",
        "* Compare between the different clustering approaches "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Normalising Iris Dataset\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(iris_data.data)\n",
        "# scaled_iris_data = scaler.transform(iris_data.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #KMeans for iris dataset\n",
        "\n",
        "# k_val = range(2, 10)  # range from 2 to 10 clusters\n",
        "\n",
        "# distortions = []\n",
        "# silhouette_scores = []\n",
        "\n",
        "# for k in k_val:\n",
        "#     kmeans = KMeans(n_clusters=k, init='k-means++', tol=0.0001, random_state=45)\n",
        "#     kmeans.fit(scaled_iris_data)  # Fit KMeans model\n",
        "#     distortions.append(kmeans.inertia_) \n",
        "#     silhouette_avg = silhouette_score(scaled_iris_data, kmeans.labels_)  \n",
        "#     silhouette_scores.append(silhouette_avg) \n",
        "#     cluster_centers = kmeans.cluster_centers_\n",
        "#     labels = kmeans.predict(scaled_iris_data)\n",
        "    \n",
        "\n",
        "# # Plot graph of distortion against k\n",
        "# plt.figure(figsize=(6, 4))\n",
        "# plt.plot(k_val, distortions, marker='o', linestyle='-')\n",
        "# plt.title('Distortion vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Distortion')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(k_val)\n",
        "# plt.show()\n",
        "# # Plot graph for silhouette against k\n",
        "# plt.figure(figsize=(6, 4))\n",
        "# plt.plot(k_val, silhouette_scores, marker='o', linestyle='-')\n",
        "# plt.title('Silhouette Score vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(k_val)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Euclidean Distance\n",
        "\n",
        "# # Euclidean and average\n",
        "# plot_dendogram(scaled_iris_data, 'euclidean', 'average')\n",
        "# Hierarchal(scaled_iris_data, np.arange(0.5, 4, 0.5), 'euclidean', 'average' )\n",
        "\n",
        "# # Euclidean and single\n",
        "# plot_dendogram(scaled_iris_data, 'euclidean', 'single')\n",
        "# Hierarchal(scaled_iris_data, np.arange(0.1, 1.7, 0.2), 'euclidean', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Manhattan Distance\n",
        "\n",
        "# # Manhattan and average\n",
        "# plot_dendogram(scaled_iris_data, 'cityblock', 'average')\n",
        "# Hierarchal(scaled_iris_data, np.arange(1, 7, 1), 'manhattan', 'average' )\n",
        "\n",
        "# # Manhattan and single\n",
        "# plot_dendogram(scaled_iris_data, 'cityblock', 'single')\n",
        "# Hierarchal(scaled_iris_data, np.arange(0.1, 2.4, 0.1), 'manhattan', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cosine Distance\n",
        "\n",
        "# # Cosine and average\n",
        "# plot_dendogram(scaled_iris_data, 'cosine', 'average')\n",
        "# Hierarchal(scaled_iris_data, np.arange(0.1, 1.6, 0.1), 'cosine', 'average' )\n",
        "\n",
        "# # Cosine and single\n",
        "# plot_dendogram(scaled_iris_data, 'cosine', 'single')\n",
        "# Hierarchal(scaled_iris_data, np.arange(0.01, 0.12, 0.01), 'cosine', 'single' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "# n_comp = range(1, 8)\n",
        "\n",
        "# for comp in n_comp:\n",
        "#     for i, cov_type in enumerate(covariance_types):\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "\n",
        "#         for feature_idx in range(4):\n",
        "#             for other_idx in range(4):\n",
        "#                 if feature_idx != other_idx and other_idx > feature_idx:\n",
        "#                     # Create new figure for each pair of features\n",
        "#                     plt.figure(figsize=(8, 6))\n",
        "\n",
        "#                     # Fit the Gaussian Mixture Model\n",
        "#                     gmm.fit(scaled_iris_data[:, [feature_idx, other_idx]])\n",
        "\n",
        "#                     # Create grid to evaluate model\n",
        "#                     x = np.linspace(scaled_iris_data[:, feature_idx].min() - 1, scaled_iris_data[:, feature_idx].max() + 1)\n",
        "#                     y = np.linspace(scaled_iris_data[:, other_idx].min() - 1, scaled_iris_data[:, other_idx].max() + 1)\n",
        "#                     X, Y = np.meshgrid(x, y)\n",
        "#                     XX = np.column_stack([X.ravel(), Y.ravel()])\n",
        "\n",
        "#                     # Score samples and reshape\n",
        "#                     Z = -gmm.score_samples(XX)\n",
        "#                     Z = Z.reshape(X.shape)\n",
        "\n",
        "#                     # Plot contour\n",
        "#                     plt.scatter(scaled_iris_data[:, feature_idx], scaled_iris_data[:, other_idx], s=40, c=gmm.predict(scaled_iris_data[:, [feature_idx, other_idx]]), cmap='viridis', zorder=2)\n",
        "#                     plt.contour(X, Y, Z, levels=10, linewidths=1, colors='black', zorder=1)\n",
        "#                     plt.title(f'GMM with Covariance Type: {cov_type}\\nFeatures: {iris_data.feature_names[feature_idx]} vs {iris_data.feature_names[other_idx]}')\n",
        "#                     plt.xlabel(iris_data.feature_names[feature_idx])\n",
        "#                     plt.ylabel(iris_data.feature_names[other_idx])\n",
        "                    \n",
        "#                     plt.tight_layout()\n",
        "#                     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# silhouette_scores = []\n",
        "# n_comp = range(2, 8)\n",
        "# for comp in n_comp:\n",
        "#     comp_scores = []\n",
        "#     for cov_type in covariance_types:\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "#         features_data = scaled_iris_data[:, :7]  # Assuming first 7 columns are features\n",
        "\n",
        "#         # Calculate silhouette score\n",
        "#         silhouette_avg = silhouette_score(features_data, gmm.fit_predict(features_data))\n",
        "#         comp_scores.append(silhouette_avg)\n",
        "\n",
        "#     silhouette_scores.append(comp_scores)\n",
        "\n",
        "# # Print silhouette scores\n",
        "# for i, comp in enumerate(n_comp):\n",
        "#     for j, cov_type in enumerate(covariance_types):\n",
        "#         print(f\"Number of components: {comp}, Covariance type: {cov_type}, Silhouette score: {silhouette_scores[i][j]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2oBmWT2udcA"
      },
      "source": [
        "# Customer dataset\n",
        "Repeat all the above on the customer data set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# def display_clust(ax, X, km=[], num_clusters=0):\n",
        "#     colors = plt.cm.tab20(np.linspace(0, 1, 20))  # List colors\n",
        "#     alpha = 0.5  # Color opacity\n",
        "#     s = 20\n",
        "#     if num_clusters == 0:\n",
        "#         ax.scatter(X[:, 0], X[:, 1], c=colors[0], alpha=alpha, s=s)\n",
        "#     else:\n",
        "#         for i in range(num_clusters):\n",
        "#             ax.scatter(X[km==i, 0], X[km==i, 1], c=colors[i], alpha=alpha, s=s)\n",
        "\n",
        "# def display_multi(dataframe, km=[], num_clusters=0):\n",
        "#     columns = dataframe.columns\n",
        "#     num_columns = len(columns)\n",
        "#     num_plots = num_columns * (num_columns - 1) // 2\n",
        "#     num_rows = num_plots // 2 + num_plots % 2  # Calculate the number of rows needed\n",
        "\n",
        "#     fig, axes = plt.subplots(num_rows, 2, figsize=(10, num_rows * 5))\n",
        "\n",
        "#     plot_idx = 0\n",
        "#     for i in range(num_columns):\n",
        "#         for j in range(i + 1, num_columns):\n",
        "#             X = dataframe[[columns[i], columns[j]]].values\n",
        "#             ax_row = plot_idx // 2  # Calculate the row index\n",
        "#             ax_col = plot_idx % 2   # Calculate the column index\n",
        "#             ax = axes[ax_row, ax_col] if num_plots > 1 else axes\n",
        "#             if num_clusters == 0:\n",
        "#                 display_clust(ax, X)\n",
        "#             else:\n",
        "#                 display_clust(ax, X, km, num_clusters)\n",
        "#             ax.set_xlabel(columns[i])\n",
        "#             ax.set_ylabel(columns[j])\n",
        "#             plot_idx += 1\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# file_path = 'data.csv'\n",
        "# df = pd.read_csv(file_path)\n",
        "# df.drop(df.columns[0], axis=1, inplace=True)\n",
        "# customer_data = df.values\n",
        "# display_multi(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unnormalized Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #KMeans for iris dataset\n",
        "\n",
        "# k_vals = range(2, 10)  # range from 2 to 10 clusters\n",
        "\n",
        "# distortions = []\n",
        "# silhouette_scores = []\n",
        "\n",
        "# for k in k_vals:\n",
        "#     kmeans = KMeans(n_clusters=k, init='k-means++', tol=0.0001, random_state=45)\n",
        "#     kmeans.fit(customer_data)  # Fit KMeans model\n",
        "#     distortions.append(kmeans.inertia_) \n",
        "#     silhouette_avg = silhouette_score(customer_data, kmeans.labels_)  \n",
        "#     silhouette_scores.append(silhouette_avg) \n",
        "#     cluster_centers = kmeans.cluster_centers_\n",
        "#     labels = kmeans.predict(customer_data)\n",
        "#     display_multi(df, labels, k)\n",
        "    \n",
        "\n",
        "# # Plot graph of distortion against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(k_vals, distortions, marker='o', linestyle='-')\n",
        "# plt.title('Distortion vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Distortion')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(k_vals)\n",
        "# plt.show()\n",
        "# # Plot graph for silhouette against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(k_vals, silhouette_scores, marker='o', linestyle='-')\n",
        "# plt.title('Silhouette Score vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(k_vals)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heirarchal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Euclidean Distance\n",
        "\n",
        "# # Euclidean and average\n",
        "# plot_dendogram(customer_data, 'euclidean', 'average')\n",
        "# Hierarchal(customer_data, np.arange(10000, 160000, 10000), 'euclidean', 'average' )\n",
        "\n",
        "# # Euclidean and single\n",
        "# plot_dendogram(customer_data, 'euclidean', 'single')\n",
        "# Hierarchal(customer_data, np.arange(1000, 10000,1000), 'euclidean', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Manhattan Distance\n",
        "\n",
        "# # Manhattan and average\n",
        "# plot_dendogram(customer_data, 'cityblock', 'average')\n",
        "# Hierarchal(customer_data, np.arange(10000, 120000, 10000), 'manhattan', 'average' )\n",
        "\n",
        "# # Manhattan and single\n",
        "# plot_dendogram(customer_data, 'cityblock', 'single')\n",
        "# Hierarchal(customer_data, np.arange(1000, 20000, 2000), 'manhattan', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cosine Distance\n",
        "\n",
        "# # Cosine and average\n",
        "# plot_dendogram(customer_data, 'cosine', 'average')\n",
        "# Hierarchal(customer_data, np.arange(0.1*10**-7, 1.6*10**-7, 0.2*10**-7), 'cosine', 'average' )\n",
        "\n",
        "# # Cosine and single\n",
        "# plot_dendogram(customer_data, 'cosine', 'single')\n",
        "# Hierarchal(customer_data, np.arange(0.1*10**-9, 1.7*10**-9, 0.2*10**-9), 'cosine', 'single' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "# n_comp = range(1, 8)\n",
        "\n",
        "# for comp in n_comp:\n",
        "#     for i, cov_type in enumerate(covariance_types):\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "\n",
        "#         for feature_idx in range(7):\n",
        "#             for other_idx in range(7):\n",
        "#                 if feature_idx != other_idx and other_idx > feature_idx:\n",
        "#                     # Create new figure for each pair of features\n",
        "#                     plt.figure(figsize=(8, 6))\n",
        "\n",
        "#                     # Fit the Gaussian Mixture Model\n",
        "#                     gmm.fit(customer_data[:, [feature_idx, other_idx]])\n",
        "\n",
        "#                     # Create grid to evaluate model\n",
        "#                     x = np.linspace(customer_data[:, feature_idx].min() - 1, customer_data[:, feature_idx].max() + 1)\n",
        "#                     y = np.linspace(customer_data[:, other_idx].min() - 1, customer_data[:, other_idx].max() + 1)\n",
        "#                     X, Y = np.meshgrid(x, y)\n",
        "#                     XX = np.column_stack([X.ravel(), Y.ravel()])\n",
        "\n",
        "#                     # Score samples and reshape\n",
        "#                     Z = -gmm.score_samples(XX)\n",
        "#                     Z = Z.reshape(X.shape)\n",
        "\n",
        "#                     # Plot contour\n",
        "#                     plt.scatter(customer_data[:, feature_idx], customer_data[:, other_idx], s=40, c=gmm.predict(customer_data[:, [feature_idx, other_idx]]), cmap='viridis', zorder=2)\n",
        "#                     plt.contour(X, Y, Z, levels=10, linewidths=1, colors='black', zorder=1)\n",
        "#                     plt.title(f'GMM with Covariance Type: {cov_type}\\nFeatures: {column_names[feature_idx]} vs {column_names[other_idx]}')\n",
        "#                     plt.xlabel(column_names[feature_idx])\n",
        "#                     plt.ylabel(column_names[other_idx])\n",
        "                    \n",
        "#                     plt.tight_layout()\n",
        "#                     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# silhouette_scores = []\n",
        "# n_comp = range(2, 8)\n",
        "# for comp in n_comp:\n",
        "#     comp_scores = []\n",
        "#     for cov_type in covariance_types:\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "#         features_data = customer_data[:, :7]  # Assuming first 7 columns are features\n",
        "\n",
        "#         # Calculate silhouette score\n",
        "#         silhouette_avg = silhouette_score(features_data, gmm.fit_predict(features_data))\n",
        "#         comp_scores.append(silhouette_avg)\n",
        "\n",
        "#     silhouette_scores.append(comp_scores)\n",
        "\n",
        "# # Print silhouette scores\n",
        "# for i, comp in enumerate(n_comp):\n",
        "#     for j, cov_type in enumerate(covariance_types):\n",
        "#         print(f\"Number of components: {comp}, Covariance type: {cov_type}, Silhouette score: {silhouette_scores[i][j]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define range of parameters\n",
        "# eps_arr = np.arange(0.1, 3.1, 0.2)\n",
        "# min_samp = np.arange(5, 26, 5)\n",
        "\n",
        "# # Iterate over parameter combinations\n",
        "# for eps in eps_arr:\n",
        "#     for min_samples in min_samp:\n",
        "#         # Create a new figure for each combination\n",
        "#         plt.figure(figsize=(8, 6))\n",
        "\n",
        "#         # Perform DBSCAN clustering\n",
        "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "#         labels = dbscan.fit_predict(Multi_blob_Data)\n",
        "#         unique_labels = np.unique(labels)\n",
        "#         clusters = len(unique_labels) - 1\n",
        "#         # Check if there are at least 2 clusters\n",
        "#         if len(unique_labels) > 1:\n",
        "#             # Calculate silhouette score\n",
        "#             score = silhouette_score(Multi_blob_Data, labels)\n",
        "\n",
        "#             # Plot clusters\n",
        "#             colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
        "#             for k, color in zip(unique_labels, colors):\n",
        "#                 class_member_mask = (labels == k)\n",
        "#                 xy = Multi_blob_Data[class_member_mask]\n",
        "#                 plt.scatter(xy[:, 0], xy[:, 1], c=[color], s=10)\n",
        "\n",
        "#             plt.title(f'EPS={eps}, Min Samples={min_samples}, Cluster={clusters}\\nSilhouette Score: {score:.2f}\\nlables={unique_labels}')\n",
        "#             plt.colorbar(label='Cluster Label')\n",
        "#             plt.grid(True)\n",
        "#             plt.show()\n",
        "            \n",
        "# print(\"Best silhouette score:\", best_score)\n",
        "# print(\"Best parameters:\", best_params)\n",
        "# silhouette_scores = np.array(silhouette_scores)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.scatter(silhouette_scores[:, 0], silhouette_scores[:, 1], c=silhouette_scores[:, 2], cmap='viridis')\n",
        "# plt.colorbar(label='Silhouette Score')\n",
        "# plt.xlabel('EPS')\n",
        "# plt.ylabel('Min Samples')\n",
        "# plt.title('Silhouette Score vs. EPS and Min Samples')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# print(\"Best silhouette score:\", best_score)\n",
        "# print(\"Best parameters:\", best_params)\n",
        "# # Plot clusters using best parameters\n",
        "# dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "# best_labels = dbscan.fit_predict(Multi_blob_Data)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=best_labels, cmap='viridis', alpha=0.7, s=50)\n",
        "# plt.title('DBSCAN Clustering with Best Parameters')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "# plt.colorbar(label='Cluster Label')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalized Customer dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Normalising Iris Dataset\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(customer_data)\n",
        "# scaled_customer_data = scaler.transform(customer_data)\n",
        "# column_names = df.columns.tolist()\n",
        "# scaled_df = pd.DataFrame(scaled_customer_data, columns=column_names)\n",
        "# print(scaled_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #KMeans for iris dataset\n",
        "\n",
        "# k_vals = range(2, 10)  # range from 2 to 10 clusters\n",
        "\n",
        "# distortions = []\n",
        "# silhouette_scores = []\n",
        "\n",
        "# for k in k_vals:\n",
        "#     kmeans = KMeans(n_clusters=k, init='k-means++', tol=0.0001, random_state=45)\n",
        "#     kmeans.fit(scaled_customer_data)  # Fit KMeans model\n",
        "#     distortions.append(kmeans.inertia_) \n",
        "#     silhouette_avg = silhouette_score(scaled_customer_data, kmeans.labels_)  \n",
        "#     silhouette_scores.append(silhouette_avg) \n",
        "#     cluster_centers = kmeans.cluster_centers_\n",
        "#     labels = kmeans.predict(scaled_customer_data)\n",
        "#     display_multi(scaled_df, labels, k)\n",
        "    \n",
        "\n",
        "# # Plot graph of distortion against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(k_vals, distortions, marker='o', linestyle='-')\n",
        "# plt.title('Distortion vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Distortion')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(k_vals)\n",
        "# plt.show()\n",
        "# # Plot graph for silhouette against k\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(k_vals, silhouette_scores, marker='o', linestyle='-')\n",
        "# plt.title('Silhouette Score vs. Number of Clusters')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.grid(True)\n",
        "# plt.xticks(k_vals)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heirarchal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Euclidean Distance\n",
        "\n",
        "# # Euclidean and average\n",
        "# plot_dendogram(scaled_customer_data, 'euclidean', 'average')\n",
        "# Hierarchal(scaled_customer_data, np.arange(1, 7, 1), 'euclidean', 'average' )\n",
        "\n",
        "# # Euclidean and single\n",
        "# plot_dendogram(scaled_customer_data, 'euclidean', 'single')\n",
        "# Hierarchal(scaled_customer_data, np.arange(0.5, 2.5, 0.5), 'euclidean', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Manhattan Distance\n",
        "\n",
        "# # Manhattan and average\n",
        "# plot_dendogram(scaled_customer_data, 'cityblock', 'average')\n",
        "# Hierarchal(scaled_customer_data, np.arange(1, 10, 1), 'manhattan', 'average' )\n",
        "\n",
        "# # Manhattan and single\n",
        "# plot_dendogram(scaled_customer_data, 'cityblock', 'single')\n",
        "# Hierarchal(scaled_customer_data, np.arange(1, 5, 1), 'manhattan', 'single' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cosine Distance\n",
        "\n",
        "# # Cosine and average\n",
        "# plot_dendogram(scaled_customer_data, 'cosine', 'average')\n",
        "# Hierarchal(scaled_customer_data, np.arange(0.1,1.2 , 0.1), 'cosine', 'average' )\n",
        "\n",
        "# # Cosine and single\n",
        "# plot_dendogram(scaled_customer_data, 'cosine', 'single')\n",
        "# Hierarchal(scaled_customer_data, np.arange(0.02, 0.24, 0.02), 'cosine', 'single' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "# n_comp = range(1, 8)\n",
        "\n",
        "# for comp in n_comp:\n",
        "#     for i, cov_type in enumerate(covariance_types):\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "\n",
        "#         for feature_idx in range(7):\n",
        "#             for other_idx in range(7):\n",
        "#                 if feature_idx != other_idx and other_idx > feature_idx:\n",
        "#                     # Create new figure for each pair of features\n",
        "#                     plt.figure(figsize=(8, 6))\n",
        "\n",
        "#                     # Fit the Gaussian Mixture Model\n",
        "#                     gmm.fit(scaled_customer_data[:, [feature_idx, other_idx]])\n",
        "\n",
        "#                     # Create grid to evaluate model\n",
        "#                     x = np.linspace(scaled_customer_data[:, feature_idx].min() - 1, scaled_customer_data[:, feature_idx].max() + 1)\n",
        "#                     y = np.linspace(scaled_customer_data[:, other_idx].min() - 1, scaled_customer_data[:, other_idx].max() + 1)\n",
        "#                     X, Y = np.meshgrid(x, y)\n",
        "#                     XX = np.column_stack([X.ravel(), Y.ravel()])\n",
        "\n",
        "#                     # Score samples and reshape\n",
        "#                     Z = -gmm.score_samples(XX)\n",
        "#                     Z = Z.reshape(X.shape)\n",
        "\n",
        "#                     # Plot contour\n",
        "#                     plt.scatter(scaled_customer_data[:, feature_idx], scaled_customer_data[:, other_idx], s=40, c=gmm.predict(scaled_customer_data[:, [feature_idx, other_idx]]), cmap='viridis', zorder=2)\n",
        "#                     plt.contour(X, Y, Z, levels=10, linewidths=1, colors='black', zorder=1)\n",
        "#                     plt.title(f'GMM with Covariance Type: {cov_type}\\nFeatures: {column_names[feature_idx]} vs {column_names[other_idx]}')\n",
        "#                     plt.xlabel(column_names[feature_idx])\n",
        "#                     plt.ylabel(column_names[other_idx])\n",
        "                    \n",
        "#                     plt.tight_layout()\n",
        "#                     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# silhouette_scores = []\n",
        "# n_comp = range(2, 8)\n",
        "# for comp in n_comp:\n",
        "#     comp_scores = []\n",
        "#     for cov_type in covariance_types:\n",
        "#         # Fit a Gaussian Mixture Model\n",
        "#         gmm = GaussianMixture(n_components=comp, covariance_type=cov_type, random_state=0)\n",
        "#         features_data = scaled_customer_data[:, :7]  # Assuming first 7 columns are features\n",
        "\n",
        "#         # Calculate silhouette score\n",
        "#         silhouette_avg = silhouette_score(features_data, gmm.fit_predict(features_data))\n",
        "#         comp_scores.append(silhouette_avg)\n",
        "\n",
        "#     silhouette_scores.append(comp_scores)\n",
        "\n",
        "# # Print silhouette scores\n",
        "# for i, comp in enumerate(n_comp):\n",
        "#     for j, cov_type in enumerate(covariance_types):\n",
        "#         print(f\"Number of components: {comp}, Covariance type: {cov_type}, Silhouette score: {silhouette_scores[i][j]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Clustering Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
